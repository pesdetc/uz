"""–ú–æ–¥—É–ª—å –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø—Ä–æ—Ñ–∏–ª–µ–π —á–µ—Ä–µ–∑ Google"""

import requests
from bs4 import BeautifulSoup
import time
import re
from typing import List, Dict
import config


class GoogleSearcher:
    """–ü–æ–∏—Å–∫ –ø—Ä–æ—Ñ–∏–ª–µ–π Telegram –∏ Instagram —á–µ—Ä–µ–∑ Google"""
    
    def __init__(self):
        self.headers = {'User-Agent': config.USER_AGENT}
        self.session = requests.Session()
    
    def search(self, query: str, max_results: int = 50) -> List[str]:
        """
        –ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É —á–µ—Ä–µ–∑ Google
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            max_results: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö URL
        """
        urls = []
        num_pages = (max_results // 10) + 1  # Google –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç ~10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
        
        for page in range(num_pages):
            try:
                start = page * 10
                search_url = f"https://www.google.com/search?q={query}&start={start}"
                
                print(f"  –ü–æ–∏—Å–∫: {query} (—Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page + 1})")
                
                response = self.session.get(search_url, headers=self.headers, timeout=10)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
                for link in soup.find_all('a'):
                    href = link.get('href', '')
                    
                    # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Å—ã–ª–∫–∏ Google
                    if '/url?q=' in href:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–π URL
                        match = re.search(r'/url\?q=(.*?)&', href)
                        if match:
                            url = match.group(1)
                            if self._is_valid_url(url):
                                urls.append(url)
                
                # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                time.sleep(config.REQUEST_DELAY)
                
                if len(urls) >= max_results:
                    break
                    
            except Exception as e:
                print(f"  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
                continue
        
        return urls[:max_results]
    
    def _is_valid_url(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ URL"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ Telegram –∏–ª–∏ Instagram
        return ('t.me/' in url or 'instagram.com/' in url) and url.startswith('http')
    
    def search_all_sources(self) -> Dict[str, List[str]]:
        """
        –ü–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º (Telegram –∏ Instagram)
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏: {'telegram': [...], 'instagram': [...]}
        """
        results = {}
        
        for source, query in config.SEARCH_QUERIES.items():
            print(f"\nüîç –ü–æ–∏—Å–∫ {source}...")
            urls = self.search(query, config.MAX_RESULTS_PER_SOURCE)
            results[source] = urls
            print(f"  –ù–∞–π–¥–µ–Ω–æ: {len(urls)} URL")
        
        return results
